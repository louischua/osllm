# üöÄ Hugging Face Spaces Guide

## üìã Overview

OpenLLM provides **two distinct Hugging Face Spaces** that serve different purposes in the AI language model ecosystem:

1. **[`lemms/llm`](https://huggingface.co/spaces/lemms/llm)** - **Live Demo Space** for model inference and comparison
2. **[`lemms/openllm`](https://huggingface.co/spaces/lemms/openllm)** - **Live Training Space** for interactive model training

---

## üéØ Space 1: `lemms/llm` - Live Demo Space

### **Purpose**
Interactive demonstration and testing of **pre-trained OpenLLM models** for text generation and model comparison.

### **What You Can Do**
- **Generate text** using 7 different trained models
- **Compare model performance** across different training steps
- **Adjust generation parameters** (temperature, max length, etc.)
- **Learn about model quality** through hands-on experimentation

### **Available Models**
| Model | Training Steps | Best Loss | Perplexity | Quality Level |
|-------|---------------|-----------|------------|---------------|
| **4k Model** | 4,000 | ~6.2 | ~492 | Basic understanding |
| **6k Model** | 6,000 | ~5.8 | ~816 | Getting better |
| **7k Model** | 7,000 | ~5.5 | ~8.2 | Much improved |
| **8k Model** | 8,000 | ~5.3 | ~200 | Sophisticated |
| **9k Model** | 9,000 | ~5.2 | ~177 | **Best performance** |
| **10k Model** | 10,000 | ~5.22 | ~184 | Extended training |
| **10k Improved** | 10,000 | ~5.1774 | ~177 | Enhanced process |

### **User Experience**
```python
# Example workflow:
1. Select a model (e.g., "9k Model - Best Performance")
2. Input a prompt: "The future of artificial intelligence is"
3. Adjust parameters:
   - Temperature: 0.7 (creativity level)
   - Max Length: 100 (response length)
   - Top-k: 40 (sampling diversity)
4. Generate text and see the AI response
5. Compare with other models to see quality differences
```

### **Educational Value**
- **Understanding model progression** from basic to advanced training
- **Learning about generation parameters** and their effects
- **Seeing real-world AI text generation** in action
- **Comparing different training approaches** and outcomes

---

## üîß Space 2: `lemms/openllm` - Live Training Space

### **Purpose**
Interactive **live training demonstration** where users can train new models from existing checkpoints with customizable settings.

### **What You Can Do**
- **Start training** from the latest model checkpoint (e.g., 9k model)
- **Configure training parameters** in real-time
- **Monitor training progress** with live metrics
- **Download or deploy** newly trained models

### **Training Configuration Options**
```python
# Available training parameters:
training_config = {
    "base_model": "lemms/openllm-small-extended-9k",  # Start from best model
    
    # Learning parameters
    "learning_rate": [1e-4, 3e-4, 5e-4, 1e-3],       # How fast to learn
    "batch_size": [4, 8, 16, 32],                     # Training batch size
    "training_steps": [1000, 2000, 5000, 10000],      # How long to train
    
    # Optimization settings
    "gradient_accumulation": [1, 2, 4, 8],            # Memory optimization
    "optimizer": ["AdamW", "Adam", "SGD"],            # Optimization algorithm
    "scheduler": ["Cosine", "Linear", "Constant"],    # Learning rate schedule
    
    # Advanced options
    "weight_decay": [0.01, 0.1, 0.0],                 # Regularization
    "gradient_clipping": [0.5, 1.0, 2.0],             # Gradient stability
    "warmup_steps": [100, 500, 1000]                  # Learning rate warmup
}
```

### **Real-Time Monitoring**
- **Live loss curves** showing training progress
- **Perplexity tracking** for model quality assessment
- **Memory usage** monitoring and optimization
- **Training speed** metrics (steps/second)
- **Validation metrics** (when available)

### **Training Scenarios**

#### **Quick Experiments (1000 steps)**
- **Duration**: 10-30 minutes
- **Purpose**: Test different learning rates and configurations
- **Use case**: Hyperparameter exploration and rapid prototyping

#### **Medium Training (5000 steps)**
- **Duration**: 1-3 hours
- **Purpose**: Significant model improvement and fine-tuning
- **Use case**: Model optimization and performance enhancement

#### **Extended Training (10000 steps)**
- **Duration**: 3-8 hours
- **Purpose**: Maximum performance improvement
- **Use case**: Production model development and research

### **Model Management**
- **Save checkpoints** during training to prevent data loss
- **Download trained models** for local use and deployment
- **Deploy new models** directly to Hugging Face Hub
- **Compare training runs** with different settings and configurations

---

## üéì Educational Value

### **For Students**
- **Hands-on training experience** without local setup requirements
- **Understanding hyperparameters** and their effects on model performance
- **Real-time observation** of training dynamics and convergence
- **Learning best practices** for language model training

### **For Researchers**
- **Rapid experimentation** with different training configurations
- **A/B testing** of training strategies and approaches
- **Parameter optimization** studies and analysis
- **Training process visualization** and analysis

### **For Developers**
- **Testing training configurations** before local deployment
- **Understanding resource requirements** for different training settings
- **Learning from training failures** and successful approaches
- **Optimizing training pipelines** and workflows

---

## üîÑ Complete AI Workflow

### **The Full Cycle**
1. **Training** (`lemms/openllm`) ‚Üí Train new models with custom settings
2. **Evaluation** (`lemms/llm`) ‚Üí Test and compare model performance
3. **Deployment** ‚Üí Deploy models to Hugging Face Hub
4. **Usage** ‚Üí Use models for text generation and applications

### **Learning Progression**
```
Beginner: Use lemms/llm to understand model performance
    ‚Üì
Intermediate: Use lemms/openllm to experiment with training
    ‚Üì
Advanced: Combine both spaces for comprehensive AI development
```

---

## üöÄ Technical Implementation

### **Space Architecture**
```python
# lemms/llm (Demo Space)
class ModelDemoInterface:
    def __init__(self):
        self.models = self.load_trained_models()
        self.gradio_interface = self.create_demo_interface()
    
    def generate_text(self, model_name, prompt, parameters):
        model = self.models[model_name]
        return model.generate(prompt, **parameters)

# lemms/openllm (Training Space)
class LiveTrainingInterface:
    def __init__(self):
        self.base_model = "lemms/openllm-small-extended-9k"
        self.training_configs = self.load_training_options()
    
    def start_training(self, config):
        trainer = ImprovedModelTrainer(
            base_model=self.base_model,
            **config
        )
        return trainer.train_with_monitoring()
```

### **Resource Management**
- **GPU allocation** for training jobs and inference
- **Memory optimization** with gradient checkpointing and quantization
- **Checkpoint saving** to prevent data loss during training
- **Queue management** for multiple users and training sessions

---

## üìä Performance Comparison

| Aspect | `lemms/llm` (Demo) | `lemms/openllm` (Training) |
|--------|-------------------|---------------------------|
| **Response Time** | Instant (seconds) | Real-time (minutes to hours) |
| **Resource Usage** | Lightweight inference | GPU-intensive training |
| **User Interaction** | Text generation | Training configuration |
| **Output** | Generated text | Trained model checkpoints |
| **Learning Focus** | Model performance | Training process |
| **Use Case** | Testing and comparison | Development and experimentation |

---

## üéØ Getting Started

### **For Model Testing (lemms/llm)**
1. Visit [https://huggingface.co/spaces/lemms/llm](https://huggingface.co/spaces/lemms/llm)
2. Select a model from the dropdown
3. Enter your prompt
4. Adjust generation parameters
5. Generate and compare results

### **For Model Training (lemms/openllm)**
1. Visit [https://huggingface.co/spaces/lemms/openllm](https://huggingface.co/spaces/lemms/openllm)
2. Configure training parameters
3. Start training session
4. Monitor progress in real-time
5. Download or deploy your trained model

---

## üîó Related Resources

- **[OpenLLM GitHub Repository](https://github.com/louischua/osllm)** - Source code and documentation
- **[Model Repository](https://huggingface.co/lemms)** - All trained models
- **[Training Documentation](docs/TRAINING_IMPROVEMENTS.md)** - Detailed training guide
- **[Project Structure](docs/PROJECT_STRUCTURE_OPTIMIZATION_SUMMARY.md)** - Codebase organization

---

## üìù License

Both spaces are part of the OpenLLM project and are available under the GPLv3 license for open source use, with commercial licensing options available.

---

**This dual-space approach provides a complete OpenLLM experience - from training models to using them for text generation, making AI language model development accessible to everyone! üöÄ**
