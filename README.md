# OpenLLM: Open Source Large Language Model

<!-- Copyright (C) 2024 Louis Chua Bean Chong -->
<!-- This file is part of OpenLLM - dual-licensed under GPLv3 and Commercial License -->

## ğŸŒŸ Overview

OpenLLM is an open source project to develop a powerful, flexible, and modular large language model (LLM) that is openly licensed under GPLv3 for research and community use, with a commercial license available for enterprise applications.

## ğŸš€ Key Features

- âœ”ï¸ Pretraining and fine-tuning pipeline
- âœ”ï¸ Tokenizer training with SentencePiece or BPE
- âœ”ï¸ Support for multilingual datasets
- âœ”ï¸ Transformer-based architecture (GPT-like)
- âœ”ï¸ Model quantization and export for inference
- âœ”ï¸ Integration with Hugging Face, PyTorch, and ONNX
- âœ”ï¸ CLI and RESTful API for inference
- ğŸ”’ Enterprise: RLHF trainer, fine-tuning UI, inference server orchestration (Kubernetes)

## ğŸ§  Design Goals

- Fully transparent and reproducible LLM stack
- Plug-and-play components (tokenizer, model, trainer)
- Scalable to billions of parameters
- Simple to extend with downstream tasks

## ğŸ“‚ Folder Structure

```
osllm-1/
â”œâ”€â”€ core/             # Open source components (training, tokenization, inference)
â”‚   â””â”€â”€ src/          # Python source files
â”‚       â”œâ”€â”€ download_and_prepare.py    # SQUAD dataset downloader & processor
â”‚       â””â”€â”€ train_tokenizer.py         # SentencePiece tokenizer trainer
â”œâ”€â”€ data/             # Training data and model artifacts
â”‚   â”œâ”€â”€ raw/          # Downloaded raw data (temporary)
â”‚   â”œâ”€â”€ clean/        # Processed training text
â”‚   â”‚   â””â”€â”€ training_data.txt          # ~41k Wikipedia passages from SQUAD
â”‚   â””â”€â”€ tokenizer/    # Trained tokenizer files
â”œâ”€â”€ enterprise/       # Enterprise-only modules (e.g., dashboard, RLHF UI)
â”œâ”€â”€ docs/             # Documentation and community guidelines
â”‚   â””â”€â”€ training_pipeline.md           # Complete training guide
â””â”€â”€ .github/          # GitHub config (PR template, funding, etc.)
```

## ğŸ› ï¸ Tech Stack

- Python 3.10+
- PyTorch
- Hugging Face Transformers
- SentencePiece
- FastAPI for inference API

## ğŸš€ Getting Started: Training Your Own Foundation Model

**ğŸ“š Follow the Complete Training Pipeline**

To understand how to use OpenLLM scripts to generate foundational models from scratch, please follow our comprehensive training guide:

**ğŸ‘‰ [Training Pipeline Documentation](docs/training_pipeline.md)**

This step-by-step guide covers the complete process:

### ğŸ“‹ **Pipeline Overview:**
1. **ğŸ“Š Data Preparation** - Download and process SQUAD dataset (~41k Wikipedia passages)
2. **ğŸ”¤ Tokenizer Training** - Train SentencePiece BPE tokenizer on your text corpus
3. **ğŸ—ï¸ Model Architecture** - Set up GPT-style transformer (Small/Medium/Large configs)
4. **ğŸ¯ Model Training** - Pre-train your language model with modern optimization
5. **ğŸ“Š Evaluation** - Assess model quality with perplexity and downstream tasks
6. **ğŸ“¦ Export & Deploy** - Save models for inference (PyTorch/HuggingFace/ONNX formats)

### âš¡ **Quick Start Commands:**
```bash
# 1. Prepare training data
python core/src/download_and_prepare.py

# 2. Train tokenizer  
python core/src/train_tokenizer.py --input data/clean/training_data.txt --vocab_size 32000

# 3. Train small model (recommended for beginners)
python core/src/main.py train-model --model-size small --data-file data/clean/training_data.txt --tokenizer-dir data/tokenizer --output-dir models/my-model

# 4. Evaluate trained model
python core/src/main.py evaluate --model_path models/my-model --metrics perplexity,generation

# 5. Export for inference
python core/src/main.py export --model_dir models/my-model --format huggingface --output_dir exports/
```

### ğŸ¯ **Model Sizes Available:**
- **Small (~25M params)** - Great for learning and CPU training
- **Medium (~117M params)** - Balanced performance, GPU recommended  
- **Large (~350M params)** - High quality, requires powerful GPU

### ğŸ“– **Essential Documentation:**
- **[Complete Training Guide](docs/training_pipeline.md)** - Detailed step-by-step instructions
- **[Model Architecture](core/src/model.py)** - GPT implementation details
- **[CLI Usage](core/src/main.py)** - All available commands and options

**ğŸ’¡ Pro Tip:** Start with the small model configuration to familiarize yourself with the training process, then scale up to larger models as needed!

## ğŸ’¼ Licensing

OpenLLM is **dual-licensed** to provide maximum flexibility:

### ğŸ†“ GPLv3 (Free for Open Source)
- âœ… **Perfect for:** Research, education, open source projects
- âœ… **Free to use** and modify
- âš ï¸ **Requirement:** Share modifications under GPL

### ğŸ’¼ Commercial License
- âœ… **Perfect for:** Proprietary software, SaaS, enterprise
- âœ… **No copyleft** restrictions
- âœ… **Keep modifications private**
- âœ… **Enterprise support included**

**Quick Guide:**
- **Open source project?** â†’ Use GPLv3 (free)
- **Commercial product?** â†’ Get commercial license
- **Not sure?** â†’ Start with GPLv3, upgrade later

**License Files:**
- [`LICENSE`](LICENSE) - GPL-3.0 license text (GitHub recognized)
- [`LICENSES/LICENSE-COMMERCIAL`](LICENSES/LICENSE-COMMERCIAL) - Commercial license terms
- [`docs/LICENSES.md`](docs/LICENSES.md) - Complete dual licensing guide

ğŸ’¬ **Commercial licensing:** Contact us at [louischua@gmail.com]

## ğŸ“‹ Project Roadmap & To-Do List

### âœ… **Completed Features**

#### Core Training Pipeline
- âœ… **Data Processing** - SQUAD dataset download and cleaning (~41k passages)
- âœ… **Tokenizer Training** - SentencePiece BPE tokenizer with 32k vocabulary
- âœ… **Model Architecture** - GPT-style transformer (Small/Medium/Large configs)
- âœ… **Training Loop** - Complete training with optimization, checkpointing, logging
- âœ… **Model Evaluation** - Perplexity, text generation quality, downstream tasks
- âœ… **Model Export** - PyTorch native, Hugging Face compatible, ONNX formats
- âœ… **CLI Interface** - Unified command-line tool for all operations

#### Advanced Features
- âœ… **Inference Server** - FastAPI REST API for model serving
- âœ… **Text Generation** - Advanced sampling with temperature, top-k, top-p
- âœ… **Enterprise Integration** - Plugin system for commercial-only features
- âœ… **Comprehensive Documentation** - Training pipeline, API docs, examples

#### Project Infrastructure
- âœ… **Dual Licensing** - GPL-3.0 + Commercial license structure
- âœ… **Professional Documentation** - Code of Conduct, Contributing guidelines
- âœ… **GitHub Templates** - Issue templates, PR templates
- âœ… **Copyright Attribution** - Proper licensing headers in all source files

### ğŸš§ **In Progress**

#### Model Improvements
- ğŸ”„ **Extended Training** - Scaling models to higher quality (6k+ steps)
- ğŸ”„ **Performance Optimization** - Memory efficiency and training speed
- ğŸ”„ **Hardware Support** - GPU optimization and multi-GPU training

#### Testing & Quality
- ğŸ”„ **Test Suite** - Comprehensive unit and integration tests
- ğŸ”„ **CI/CD Pipeline** - Automated testing and deployment
- ğŸ”„ **Model Benchmarking** - Standardized evaluation protocols

### ğŸ”® **Planned Features**

#### Core Enhancements
- ğŸ“ **Multi-Language Support** - Training on multilingual datasets
- ğŸ“ **Custom Datasets** - Support for user-provided training data
- ğŸ“ **Advanced Architectures** - Support for newer transformer variants
- ğŸ“ **Distributed Training** - Multi-node training for large models
- ğŸ“ **Mixed Precision** - FP16/BF16 training for efficiency

#### Advanced Training
- ğŸ“ **Fine-tuning Pipeline** - Task-specific model adaptation
- ğŸ“ **RLHF (Reinforcement Learning from Human Feedback)** - Alignment training
- ğŸ“ **Instruction Tuning** - Chat/instruction-following capabilities
- ğŸ“ **Parameter-Efficient Fine-tuning** - LoRA, AdaLoRA, QLoRA support

#### Production Features
- ğŸ“ **Model Quantization** - INT8/INT4 quantization for deployment
- ğŸ“ **Batch Inference** - Optimized batch processing
- ğŸ“ **Streaming Generation** - Real-time text streaming
- ğŸ“ **Model Caching** - Intelligent model loading and caching

#### Enterprise Features (Commercial License)
- ğŸ“ **Web Dashboard** - Training monitoring and management UI
- ğŸ“ **Kubernetes Deployment** - Scalable cloud deployment
- ğŸ“ **Advanced Analytics** - Training metrics and performance monitoring
- ğŸ“ **Enterprise Support** - Priority support and consulting
- ğŸ“ **Custom Training Services** - Professional model training assistance

#### Developer Experience
- ğŸ“ **Jupyter Notebooks** - Interactive tutorials and examples
- ğŸ“ **Docker Containers** - Pre-configured development environments
- ğŸ“ **Model Hub Integration** - Easy sharing and discovery of trained models
- ğŸ“ **Auto-Documentation** - Automated API documentation generation

#### Research & Experimentation
- ğŸ“ **Experiment Tracking** - Integration with MLflow/Weights & Biases
- ğŸ“ **Hyperparameter Optimization** - Automated hyperparameter tuning
- ğŸ“ **Architecture Search** - Neural architecture search capabilities
- ğŸ“ **Research Baselines** - Standard benchmarks and comparisons

### ğŸ¯ **Priority Milestones**

#### **v0.2.0 - Production Ready**
- Enhanced model quality and stability
- Comprehensive testing and CI/CD
- Docker containerization
- Performance optimizations

#### **v0.3.0 - Advanced Training**
- Fine-tuning capabilities
- Multi-language support
- Distributed training
- Advanced evaluation metrics

#### **v1.0.0 - Enterprise Ready**
- RLHF and instruction tuning
- Production-grade inference
- Enterprise dashboard
- Professional support services

### ğŸ¤ **How to Contribute**

We welcome contributions to any of these areas! Here's how you can help:

- **ğŸ› Bug Fixes** - Report and fix issues in existing features
- **ğŸ“ Documentation** - Improve guides, tutorials, and API docs
- **ğŸ”¬ Research** - Experiment with new architectures and training methods
- **ğŸš€ Features** - Implement items from our planned features list
- **ğŸ§ª Testing** - Add tests and improve code quality
- **ğŸ’¼ Enterprise** - Contribute to commercial-licensed features

See our [Contributing Guide](docs/CONTRIBUTING.md) for detailed instructions!

## ğŸ¤ Contributing

We welcome contributions from the community! Please read our:
- [Contributing Guide](docs/CONTRIBUTING.md) - How to contribute to OpenLLM
- [Code of Conduct](docs/CODE_OF_CONDUCT.md) - Community guidelines and standards

For questions or support, feel free to:
- ğŸ“ Open an [issue](https://github.com/louischua/openllm/issues)
- ğŸ’¬ Start a [discussion](https://github.com/louischua/openllm/discussions)
- ğŸ“§ Email us at [louischua@gmail.com]
