# OpenLLM: Open Source Large Language Model

<!-- Copyright (C) 2024 Louis Chua Bean Chong -->
<!-- This file is part of OpenLLM - dual-licensed under GPLv3 and Commercial License -->

## ğŸŒŸ Overview

OpenLLM is an open source project to develop a powerful, flexible, and modular large language model (LLM) that is openly licensed under GPLv3 for research and community use, with a commercial license available for enterprise applications.

### **ğŸ¯ Current Status**

âœ… **Pre-trained Model Available:** [lemms/openllm-small-extended-6k](https://huggingface.co/lemms/openllm-small-extended-6k)  
âœ… **Inference Server:** FastAPI-based production-ready server  
âœ… **Training Pipeline:** Complete end-to-end training workflow  
âœ… **Documentation:** Comprehensive guides and examples  
âœ… **Testing:** Model evaluation and benchmarking tools

## ğŸš€ Key Features

- âœ”ï¸ Pretraining and fine-tuning pipeline
- âœ”ï¸ Tokenizer training with SentencePiece or BPE
- âœ”ï¸ Support for multilingual datasets
- âœ”ï¸ Transformer-based architecture (GPT-like)
- âœ”ï¸ Model quantization and export for inference
- âœ”ï¸ Integration with Hugging Face, PyTorch, and ONNX
- âœ”ï¸ CLI and RESTful API for inference
- ğŸ”’ Enterprise: RLHF trainer, fine-tuning UI, inference server orchestration (Kubernetes)

## ğŸ§  Design Goals

- Fully transparent and reproducible LLM stack
- Plug-and-play components (tokenizer, model, trainer)
- Scalable to billions of parameters
- Simple to extend with downstream tasks

## ğŸ“‚ Folder Structure

```
osllm-1/
â”œâ”€â”€ compare_models.py           # Model comparison and benchmarking utility
â”œâ”€â”€ configs/                    # Model configuration files
â”‚   â”œâ”€â”€ large_model.json       # Large model hyperparameters
â”‚   â”œâ”€â”€ medium_model.json      # Medium model hyperparameters
â”‚   â””â”€â”€ small_model.json       # Small model hyperparameters
â”œâ”€â”€ core/                       # Open source components (training, tokenization, inference)
â”‚   â”œâ”€â”€ LICENSE                 # Core module license
â”‚   â”œâ”€â”€ README.md              # Core module documentation
â”‚   â””â”€â”€ src/                   # Python source files
â”‚       â”œâ”€â”€ data_loader.py              # Dataset loading and preprocessing
â”‚       â”œâ”€â”€ download_and_prepare.py     # SQUAD dataset downloader & processor
â”‚       â”œâ”€â”€ enterprise_integration.py  # Enterprise feature integration
â”‚       â”œâ”€â”€ evaluate_model.py          # Model evaluation and metrics
â”‚       â”œâ”€â”€ export_model.py            # Model export to various formats
â”‚       â”œâ”€â”€ generate_text.py           # Text generation utilities
â”‚       â”œâ”€â”€ inference_server.py        # FastAPI inference server
â”‚       â”œâ”€â”€ main.py                    # Main CLI interface
â”‚       â”œâ”€â”€ model.py                   # Transformer model architecture
â”‚       â”œâ”€â”€ test_model.py              # Model testing utilities
â”‚       â”œâ”€â”€ train_model.py             # Model training pipeline
â”‚       â””â”€â”€ train_tokenizer.py         # SentencePiece tokenizer trainer
â”œâ”€â”€ data/                       # Training data and model artifacts
â”‚   â”œâ”€â”€ raw/                    # Downloaded raw data (temporary)
â”‚   â”œâ”€â”€ clean/                  # Processed training text
â”‚   â”‚   â””â”€â”€ training_data.txt   # ~41k Wikipedia passages from SQUAD
â”‚   â””â”€â”€ tokenizer/              # Trained tokenizer files
â”‚       â”œâ”€â”€ tokenizer_config.json # Tokenizer configuration
â”‚       â”œâ”€â”€ tokenizer.model     # Trained SentencePiece model
â”‚       â””â”€â”€ tokenizer.vocab     # Vocabulary file
â”œâ”€â”€ docs/                       # Documentation and community guidelines
â”‚   â”œâ”€â”€ CODE_OF_CONDUCT.md      # Community guidelines
â”‚   â”œâ”€â”€ CONTRIBUTING.md         # Contribution guidelines
â”‚   â”œâ”€â”€ COPYRIGHT_HEADER.txt    # Standard copyright header
â”‚   â”œâ”€â”€ deployment_guide.md     # Deployment instructions
â”‚   â”œâ”€â”€ LICENSES.md            # Licensing information
â”‚   â””â”€â”€ training_pipeline.md   # Complete training guide
â”œâ”€â”€ enterprise/                 # Enterprise-only modules
â”‚   â””â”€â”€ README.md              # Enterprise features documentation
â”œâ”€â”€ exports/                    # Exported model formats
â”‚   â”œâ”€â”€ huggingface/           # Hugging Face compatible exports
â”‚   â”‚   â”œâ”€â”€ config.json        # Model configuration
â”‚   â”‚   â”œâ”€â”€ generation_config.json # Generation parameters
â”‚   â”‚   â”œâ”€â”€ load_hf_model.py   # Hugging Face loader script
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin  # Model weights
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json # Tokenizer config
â”‚   â”‚   â””â”€â”€ tokenizer.model    # Tokenizer model
â”‚   â””â”€â”€ pytorch/               # PyTorch native exports
â”‚       â”œâ”€â”€ config.json        # Model configuration
â”‚       â”œâ”€â”€ load_model.py      # PyTorch loader script
â”‚       â”œâ”€â”€ model.pt          # Model state dict
â”‚       â””â”€â”€ tokenizer.model   # Tokenizer model
â”œâ”€â”€ LICENSES/                   # License files
â”‚   â”œâ”€â”€ LICENSE-COMMERCIAL     # Commercial license terms
â”‚   â”œâ”€â”€ LICENSE-DUAL-INFO      # Dual licensing information
â”‚   â”œâ”€â”€ LICENSE-GPL-3.0        # GPL-3.0 license text
â”‚   â””â”€â”€ README.md             # License documentation
â”œâ”€â”€ models/                     # Trained models and checkpoints
â”œâ”€â”€ pyproject.toml             # Python project configuration
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ SECURITY.md               # Security policy and reporting
â””â”€â”€ test_trained_model.py     # Model testing script
```

## ğŸ› ï¸ Tech Stack

- Python 3.10+
- PyTorch
- Hugging Face Transformers
- SentencePiece
- FastAPI for inference API

## ğŸš€ Getting Started

### **ğŸ¯ Quick Start: Use Our Pre-trained Model**

We have a pre-trained OpenLLM model available on Hugging Face that you can use immediately:

**ğŸ”— Model:** [lemms/openllm-small-extended-6k](https://huggingface.co/lemms/openllm-small-extended-6k)

**ğŸ’¡ Note:** The quick start guide now downloads the model and tokenizer directly from Hugging Face, so it works for all users!

```python
# Install and use the pre-trained model
pip install torch sentencepiece huggingface_hub

# Load the model and tokenizer from Hugging Face
import torch
import sentencepiece as spm
from huggingface_hub import hf_hub_download
import sys
import os

# Add the core/src directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'core', 'src'))
from model import create_model

# Download and load tokenizer from Hugging Face
tokenizer = spm.SentencePieceProcessor()
tokenizer.load(hf_hub_download("lemms/openllm-small-extended-6k", "tokenizer.model"))

# Download and load model from Hugging Face
model = create_model("small")
checkpoint = torch.load(hf_hub_download("lemms/openllm-small-extended-6k", "pytorch_model.bin"), map_location="cpu")
model.load_state_dict(checkpoint)
model.eval()

# Generate text
text = "The future of AI"
tokens = tokenizer.encode(text)
inputs = torch.tensor([tokens])

with torch.no_grad():
    outputs = model.generate(inputs, max_new_tokens=50, temperature=0.7)
    
generated_text = tokenizer.decode(outputs[0].tolist())
print(generated_text)
```

### **ğŸ“š Documentation**

- **[ğŸ“– User Guide](docs/user-guide.md)** - Complete usage instructions and examples
- **[ğŸš€ Deployment Guide](docs/deployment-guide.md)** - Production deployment with Docker & Kubernetes
- **[ğŸ—ï¸ Training Guide](docs/training_pipeline.md)** - Train your own models from scratch
- **[ğŸ—ºï¸ Roadmap](docs/roadmap.md)** - Development roadmap and future plans

### **ğŸ“Š Model Performance**

- **Model Size:** 35.8M parameters
- **Training Steps:** 6,000
- **Context Length:** 512 tokens
- **Tokenizer:** SentencePiece BPE (32k vocabulary)

**ğŸ’¡ Pro Tip:** For production use, check out our [deployment guide](docs/deployment-guide.md) for Docker and Kubernetes setup!

## ğŸ’¼ Licensing

OpenLLM is **dual-licensed** to provide maximum flexibility:

### ğŸ†“ GPLv3 (Free for Open Source)
- âœ… **Perfect for:** Research, education, open source projects
- âœ… **Free to use** and modify
- âš ï¸ **Requirement:** Share modifications under GPL

### ğŸ’¼ Commercial License
- âœ… **Perfect for:** Proprietary software, SaaS, enterprise
- âœ… **No copyleft** restrictions
- âœ… **Keep modifications private**
- âœ… **Enterprise support included**

**Quick Guide:**
- **Open source project?** â†’ Use GPLv3 (free)
- **Commercial product?** â†’ Get commercial license
- **Not sure?** â†’ Start with GPLv3, upgrade later

**License Files:**
- [`LICENSE`](LICENSE) - GPL-3.0 license text (GitHub recognized)
- [`LICENSES/LICENSE-COMMERCIAL`](LICENSES/LICENSE-COMMERCIAL) - Commercial license terms
- [`docs/LICENSES.md`](docs/LICENSES.md) - Complete dual licensing guide

ğŸ’¬ **Commercial licensing:** Contact us at [louischua@gmail.com]

## ğŸ—ºï¸ **Development Roadmap**

For detailed information about our development plans, milestones, and future features, see our comprehensive roadmap:

**ğŸ“‹ [Complete Development Roadmap](docs/roadmap.md)**

This includes:
- âœ… **Completed Features** - What we've built so far
- ğŸš§ **In Progress** - Current development work
- ğŸ”® **Planned Features** - Future capabilities
- ğŸ¯ **Priority Milestones** - Version releases and timelines
- ğŸ† **Competitive Analysis** - Market positioning
- âš ï¸ **Risk Assessment** - Challenges and mitigation strategies

## ğŸ¤ Contributing

We welcome contributions from the community! Please read our:
- [Contributing Guide](docs/CONTRIBUTING.md) - How to contribute to OpenLLM
- [Code of Conduct](docs/CODE_OF_CONDUCT.md) - Community guidelines and standards

For questions or support, feel free to:
- ğŸ“ Open an [issue](https://github.com/louischua/openllm/issues)
- ğŸ’¬ Start a [discussion](https://github.com/louischua/openllm/discussions)
- ğŸ“§ Email us at [louischua@gmail.com]
