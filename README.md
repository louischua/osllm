# OpenLLM: Open Source Large Language Model

<!-- Copyright (C) 2024 Louis Chua Bean Chong -->
<!-- This file is part of OpenLLM - dual-licensed under GPLv3 and Commercial License -->

## ğŸŒŸ Overview

OpenLLM is an open source project to develop a powerful, flexible, and modular large language model (LLM) that is openly licensed under GPLv3 for research and community use, with a commercial license available for enterprise applications.

### **ğŸ¯ Current Status**

âœ… **Pre-trained Models Available:** 
   - [lemms/openllm-small-extended-6k](https://huggingface.co/lemms/openllm-small-extended-6k) (6,000 steps)
   - [lemms/openllm-small-extended-7k](https://huggingface.co/lemms/openllm-small-extended-7k) (7,000 steps)  
âœ… **Inference Server:** FastAPI-based production-ready server  
âœ… **Training Pipeline:** Complete end-to-end training workflow  
âœ… **Documentation:** Comprehensive guides and examples  
âœ… **Test Suite:** Comprehensive unit and integration tests

## ğŸš€ Key Features

- âœ”ï¸ Pretraining and fine-tuning pipeline
- âœ”ï¸ Tokenizer training with SentencePiece or BPE
- âœ”ï¸ Support for multilingual datasets
- âœ”ï¸ Transformer-based architecture (GPT-like)
- âœ”ï¸ Model quantization and export for inference
- âœ”ï¸ Integration with Hugging Face, PyTorch, and ONNX
- âœ”ï¸ CLI and RESTful API for inference
- ğŸ”’ Enterprise: RLHF trainer, fine-tuning UI, inference server orchestration (Kubernetes)

## ğŸ§  Design Goals

- Fully transparent and reproducible LLM stack
- Plug-and-play components (tokenizer, model, trainer)
- Scalable to billions of parameters
- Simple to extend with downstream tasks

## ğŸ“‚ Folder Structure

```
osllm-1/
â”œâ”€â”€ compare_models.py           # Model comparison and benchmarking utility
â”œâ”€â”€ configs/                    # Model configuration files
â”‚   â”œâ”€â”€ large_model.json       # Large model hyperparameters
â”‚   â”œâ”€â”€ medium_model.json      # Medium model hyperparameters
â”‚   â””â”€â”€ small_model.json       # Small model hyperparameters
â”œâ”€â”€ core/                       # Open source components (training, tokenization, inference)
â”‚   â”œâ”€â”€ LICENSE                 # Core module license
â”‚   â”œâ”€â”€ README.md              # Core module documentation
â”‚   â””â”€â”€ src/                   # Python source files
â”‚       â”œâ”€â”€ data_loader.py              # Dataset loading and preprocessing
â”‚       â”œâ”€â”€ download_and_prepare.py     # SQUAD dataset downloader & processor
â”‚       â”œâ”€â”€ enterprise_integration.py  # Enterprise feature integration
â”‚       â”œâ”€â”€ evaluate_model.py          # Model evaluation and metrics
â”‚       â”œâ”€â”€ export_model.py            # Model export to various formats
â”‚       â”œâ”€â”€ generate_text.py           # Text generation utilities
â”‚       â”œâ”€â”€ inference_server.py        # FastAPI inference server
â”‚       â”œâ”€â”€ main.py                    # Main CLI interface
â”‚       â”œâ”€â”€ model.py                   # Transformer model architecture
â”‚       â”œâ”€â”€ test_model.py              # Model testing utilities
â”‚       â”œâ”€â”€ train_model.py             # Model training pipeline
â”‚       â””â”€â”€ train_tokenizer.py         # SentencePiece tokenizer trainer
â”œâ”€â”€ data/                       # Training data and model artifacts
â”‚   â”œâ”€â”€ raw/                    # Downloaded raw data (temporary)
â”‚   â”œâ”€â”€ clean/                  # Processed training text
â”‚   â”‚   â””â”€â”€ training_data.txt   # ~41k Wikipedia passages from SQUAD
â”‚   â””â”€â”€ tokenizer/              # Trained tokenizer files
â”‚       â”œâ”€â”€ tokenizer_config.json # Tokenizer configuration
â”‚       â”œâ”€â”€ tokenizer.model     # Trained SentencePiece model
â”‚       â””â”€â”€ tokenizer.vocab     # Vocabulary file
â”œâ”€â”€ docs/                       # Documentation and community guidelines
â”‚   â”œâ”€â”€ CODE_OF_CONDUCT.md      # Community guidelines
â”‚   â”œâ”€â”€ CONTRIBUTING.md         # Contribution guidelines
â”‚   â”œâ”€â”€ COPYRIGHT_HEADER.txt    # Standard copyright header
â”‚   â”œâ”€â”€ deployment_guide.md     # Deployment instructions
â”‚   â”œâ”€â”€ LICENSES.md            # Licensing information
â”‚   â””â”€â”€ training_pipeline.md   # Complete training guide
â”œâ”€â”€ tests/                      # Comprehensive test suite
â”‚   â”œâ”€â”€ __init__.py            # Test package initialization
â”‚   â”œâ”€â”€ README.md              # Test suite documentation and guidelines
â”‚   â”œâ”€â”€ requirements-test.txt  # Test dependencies
â”‚   â”œâ”€â”€ run_tests.py           # Main test runner with coverage reporting
â”‚   â”œâ”€â”€ test_model.py          # Model architecture and configuration tests
â”‚   â”œâ”€â”€ test_training.py       # Training pipeline and data loader tests
â”‚   â””â”€â”€ test_inference.py      # Inference server and API tests
â”œâ”€â”€ enterprise/                 # Enterprise-only modules
â”‚   â””â”€â”€ README.md              # Enterprise features documentation
â”œâ”€â”€ exports/                    # Exported model formats
â”‚   â”œâ”€â”€ huggingface/           # Hugging Face compatible exports
â”‚   â”‚   â”œâ”€â”€ config.json        # Model configuration
â”‚   â”‚   â”œâ”€â”€ generation_config.json # Generation parameters
â”‚   â”‚   â”œâ”€â”€ load_hf_model.py   # Hugging Face loader script
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin  # Model weights
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json # Tokenizer config
â”‚   â”‚   â””â”€â”€ tokenizer.model    # Tokenizer model
â”‚   â””â”€â”€ pytorch/               # PyTorch native exports
â”‚       â”œâ”€â”€ config.json        # Model configuration
â”‚       â”œâ”€â”€ load_model.py      # PyTorch loader script
â”‚       â”œâ”€â”€ model.pt          # Model state dict
â”‚       â””â”€â”€ tokenizer.model   # Tokenizer model
â”œâ”€â”€ LICENSES/                   # License files
â”‚   â”œâ”€â”€ LICENSE-COMMERCIAL     # Commercial license terms
â”‚   â”œâ”€â”€ LICENSE-DUAL-INFO      # Dual licensing information
â”‚   â”œâ”€â”€ LICENSE-GPL-3.0        # GPL-3.0 license text
â”‚   â””â”€â”€ README.md             # License documentation
â”œâ”€â”€ models/                     # Trained models and checkpoints
â”œâ”€â”€ pyproject.toml             # Python project configuration
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ SECURITY.md               # Security policy and reporting
â””â”€â”€ test_trained_model.py     # Model testing script
```

## ğŸ› ï¸ Tech Stack

- Python 3.10+
- PyTorch
- Hugging Face Transformers
- SentencePiece
- FastAPI for inference API

## ğŸš€ Getting Started

### **ğŸ¯ Quick Start: Use Our Pre-trained Models**

We have multiple pre-trained OpenLLM models available on Hugging Face that you can use immediately:

**ğŸ”— Available Models:**
- **[lemms/openllm-small-extended-6k](https://huggingface.co/lemms/openllm-small-extended-6k)** (6,000 training steps)
- **[lemms/openllm-small-extended-7k](https://huggingface.co/lemms/openllm-small-extended-7k)** (7,000 training steps) - **Latest & Recommended**

**ğŸ’¡ Note:** The quick start guide downloads models directly from Hugging Face, so it works for all users!

#### **ğŸš€ Option 1: Using the Latest 7k Model (Recommended)**

```python
# Install dependencies
pip install torch sentencepiece huggingface_hub transformers

# Load the latest 7k model using Transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model and tokenizer from Hugging Face
model_name = "lemms/openllm-small-extended-7k"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Generate text
prompt = "The future of artificial intelligence"
inputs = tokenizer(prompt, return_tensors="pt")

with torch.no_grad():
    outputs = model.generate(
        inputs.input_ids,
        max_new_tokens=100,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id
    )

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

#### **ğŸ”§ Option 2: Using Custom Loader (Advanced)**

```python
# Install dependencies
pip install torch sentencepiece huggingface_hub

# Load the model using our custom loader
import torch
import sentencepiece as spm
from huggingface_hub import hf_hub_download
import sys
import os

# Add the core/src directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'core', 'src'))
from model import create_model

# Download and load tokenizer from Hugging Face
tokenizer = spm.SentencePieceProcessor()
tokenizer.load(hf_hub_download("lemms/openllm-small-extended-7k", "tokenizer.model"))

# Download and load model from Hugging Face
model = create_model("small")
checkpoint = torch.load(hf_hub_download("lemms/openllm-small-extended-7k", "pytorch_model.bin"), map_location="cpu")
model.load_state_dict(checkpoint)
model.eval()

# Generate text
text = "The future of AI"
tokens = tokenizer.encode(text)
inputs = torch.tensor([tokens])

with torch.no_grad():
    outputs = model.generate(inputs, max_new_tokens=50, temperature=0.7)
    
generated_text = tokenizer.decode(outputs[0].tolist())
print(generated_text)
```

#### **ğŸŒ Option 3: Using the Inference Server**

```bash
# Start the FastAPI inference server
python core/src/inference_server.py \
    --model_path exports/huggingface-7k/huggingface \
    --port 8000

# Make API calls
curl -X POST "http://localhost:8000/generate" \
    -H "Content-Type: application/json" \
    -d '{
        "prompt": "The future of renewable energy",
        "max_tokens": 100,
        "temperature": 0.7
    }'
```

### **ğŸ“š Documentation**

- **[ğŸ“– User Guide](docs/user-guide.md)** - Complete usage instructions and examples
- **[ğŸš€ Deployment Guide](docs/deployment-guide.md)** - Production deployment with Docker & Kubernetes
- **[ğŸ—ï¸ Training Guide](docs/training_pipeline.md)** - Train your own models from scratch
- **[ğŸ—ºï¸ Roadmap](docs/roadmap.md)** - Development roadmap and future plans

### **ğŸ§ª Testing**

Our comprehensive test suite ensures code quality and reliability:

- **[ğŸ§ª Test Suite](tests/)** - Complete test coverage for all components
- **[ğŸ“Š Test Coverage](tests/README.md)** - Detailed testing documentation and guidelines
- **[âš¡ Quick Test Run](tests/run_tests.py)** - Easy test execution with coverage reporting

**Run the tests:**
```bash
# Install test dependencies
pip install -r tests/requirements-test.txt

# Run all tests
python tests/run_tests.py

# Run specific test modules
python -m pytest tests/test_model.py -v
python -m pytest tests/test_training.py -v
python -m pytest tests/test_inference.py -v
```

**Test Coverage:**
- âœ… **Model Architecture** - GPT model, attention, and configuration tests
- âœ… **Training Pipeline** - Data loading, training loop, and evaluation
- âœ… **Inference Server** - API endpoints, text generation, and performance
- âœ… **Integration Tests** - End-to-end workflow validation

### **ğŸ“Š Model Performance & Comparison**

| Model | Parameters | Training Steps | Context Length | Use Case | Performance |
|-------|------------|----------------|----------------|----------|-------------|
| **Small 6K** | 35.8M | 6,000 | 1,024 | Basic text generation | Good coherence |
| **Small 7K** | 35.8M | 7,000 | 1,024 | **Extended training** | **Improved quality** |

**Model Specifications:**
- **Architecture:** GPT-style Transformer
- **Layers:** 6 transformer layers
- **Heads:** 8 attention heads
- **Embedding Dimension:** 512
- **Vocabulary Size:** 32,000 tokens
- **Tokenizer:** SentencePiece BPE (32k vocabulary)

**Performance Metrics:**
- **Inference Speed:** ~50 tokens/second on CPU, ~200 tokens/second on GPU
- **Memory Usage:** ~2GB VRAM during training, ~1GB for inference
- **Model Size:** 161MB (pytorch_model.bin)

**ğŸ’¡ Pro Tip:** For production use, check out our [deployment guide](docs/deployment-guide.md) for Docker and Kubernetes setup!

### **ğŸ¯ Model Capabilities & Use Cases**

**Text Generation Tasks:**
- âœ… **Paragraph Generation** - Coherent, context-aware text generation
- âœ… **Question Answering** - Basic factual responses from training data
- âœ… **Text Summarization** - Short text summarization capabilities
- âœ… **Language Understanding** - Context-aware responses and reasoning

**Recommended Applications:**
- **Research & Education** - Learning about language models and AI
- **Prototyping** - Quick development of text generation features
- **Content Creation** - Basic text generation for creative writing
- **Chatbots** - Simple conversational AI applications

**Model Limitations:**
- **Context Length:** Limited to 1,024 tokens
- **Training Data:** Wikipedia passages only (limited domain)
- **Model Size:** Small model with basic reasoning capabilities
- **Factual Accuracy:** Not guaranteed for current events

**ğŸ’¡ For Advanced Use Cases:** Consider training larger models or fine-tuning for specific domains.

### **ğŸ—ï¸ Model Development & Training**

**Training Process:**
- **Dataset:** Wikipedia passages from SQuAD dataset (~41k passages)
- **Tokenization:** SentencePiece with 32k vocabulary
- **Training Objective:** Next token prediction (causal language modeling)
- **Optimizer:** AdamW with learning rate scheduling
- **Hardware:** Consumer GPU with gradient accumulation

**Model Evolution:**
- **4K Model:** Basic training foundation
- **6K Model:** Improved coherence and quality
- **7K Model:** Extended training for better performance

**Training Metrics:**
- **Final Loss:** ~2.1 (cross-entropy)
- **Training Time:** ~7 hours on consumer GPU
- **Memory Usage:** ~2GB VRAM during training

**ğŸ”¬ Research & Development:** All training code, data processing, and model architecture are open source and fully reproducible.

### **ğŸ¤— Hugging Face Integration**

**Model Distribution:**
- **Repository:** [lemms/openllm-small-extended-7k](https://huggingface.co/lemms/openllm-small-extended-7k)
- **Format:** Fully compatible with Hugging Face Transformers
- **License:** GPL-3.0 / Commercial available
- **Documentation:** Comprehensive README with usage examples

**Easy Integration:**
```python
# One-line model loading
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("lemms/openllm-small-extended-7k")
model = AutoModelForCausalLM.from_pretrained("lemms/openllm-small-extended-7k")
```

**Community Benefits:**
- âœ… **Easy Access** - Download models directly from Hugging Face Hub
- âœ… **Standard Format** - Compatible with the entire Hugging Face ecosystem
- âœ… **Version Control** - Track model versions and improvements
- âœ… **Community Sharing** - Share and discover models easily

## ğŸ’¼ Licensing

OpenLLM is **dual-licensed** to provide maximum flexibility:

### ğŸ†“ GPLv3 (Free for Open Source)
- âœ… **Perfect for:** Research, education, open source projects
- âœ… **Free to use** and modify
- âš ï¸ **Requirement:** Share modifications under GPL

### ğŸ’¼ Commercial License
- âœ… **Perfect for:** Proprietary software, SaaS, enterprise
- âœ… **No copyleft** restrictions
- âœ… **Keep modifications private**
- âœ… **Enterprise support included**

**Quick Guide:**
- **Open source project?** â†’ Use GPLv3 (free)
- **Commercial product?** â†’ Get commercial license
- **Not sure?** â†’ Start with GPLv3, upgrade later

**License Files:**
- [`LICENSE`](LICENSE) - GPL-3.0 license text (GitHub recognized)
- [`LICENSES/LICENSE-COMMERCIAL`](LICENSES/LICENSE-COMMERCIAL) - Commercial license terms
- [`docs/LICENSES.md`](docs/LICENSES.md) - Complete dual licensing guide

ğŸ’¬ **Commercial licensing:** Contact us at [louischua@gmail.com]

## ğŸ—ºï¸ **Development Roadmap**

For detailed information about our development plans, milestones, and future features, see our comprehensive roadmap:

**ğŸ“‹ [Complete Development Roadmap](docs/roadmap.md)**

This includes:
- âœ… **Completed Features** - What we've built so far
- ğŸš§ **In Progress** - Current development work
- ğŸ”® **Planned Features** - Future capabilities
- ğŸ¯ **Priority Milestones** - Version releases and timelines
- ğŸ† **Competitive Analysis** - Market positioning
- âš ï¸ **Risk Assessment** - Challenges and mitigation strategies

## ğŸ¤ Contributing

We welcome contributions from the community! Please read our:
- [Contributing Guide](docs/CONTRIBUTING.md) - How to contribute to OpenLLM
- [Code of Conduct](docs/CODE_OF_CONDUCT.md) - Community guidelines and standards

For questions or support, feel free to:
- ğŸ“ Open an [issue](https://github.com/louischua/openllm/issues)
- ğŸ’¬ Start a [discussion](https://github.com/louischua/openllm/discussions)
- ğŸ“§ Email us at [louischua@gmail.com]
